## 날짜: 2025-03-28

### 스크럼
- 학습 목표 1 : 자동완성 기반 검색 기능의 원리 이해
- 학습 목표 2 : Elasticsearch의 Fuzzy Search 및 prefix 검색 방식 학습
- 학습 목표 3 : analyzer, tokenizer, ngram 등 검색 관련 개념 정리

### 새로 배운 내용
#### 주제 1: 검색 기능을 어떻게 만들 것인가 (1): Trie 알고리즘
- 자동완성/접두어 검색을 위해 메모리 기반 Trie 자료구조를 사용할 수 있음
- Elasticsearch나 DB 기반 검색 시스템은 자체 인덱싱이나 역색인 방식 사용
- Trie는 전체 데이터를 메모리에 올릴 수 있고 초고속 검색이 필요할 때 유용

#### 주제 2: Elasticsearch를 활용한 검색 기능
- Fuzzy Search: Levenshtein distance 기반으로 오타 허용 검색 가능
- match_phrase_prefix: 접두사 기반으로 자동완성 기능 구현 가능
- Tokenizer/Analyzer/Token Filter 구조와 역할 정리
- ngram, edge_ngram 설정을 통해 중간 포함 검색, 접두어 검색 구현 가능

### 오늘의 도전 과제와 해결 방법
- 도전 과제 1: Fuzzy 검색은 토큰 단위 분해에서는 사용할 수 없음
    - 해결 방법: nori-tokenizer를 이용하여 형태소 분해 진행. 이 경우에는 Fuzzy 검색 가능
- 도전 과제 2: ‘기름짜는 김밥집’이 검색되지 않는 문제
    - 해결 방법: fuzzy는 전체 단어 기준이라 해당 문제 발생, ngram 분석기를 적용한 필드를 따로 구성하여 중간 포함 검색 처리

### 오늘의 회고
- Elasticsearch의 분석기 설정이 단순히 토큰 분해 방식만 바꾸는 것이 아니라 검색 결과의 품질을 크게 좌우한다는 점을 체감했다.
- 실무에서는 text, keyword, ngram, prefix 등 여러 필드를 조합한 멀티 필드 매핑이 핵심임을 깨달음.
- Redis를 검색어 캐싱에 활용하는 방법도 함께 고민해볼 필요가 있음. 다음에는 autocomplete API 설계에 Redis TTL 적용을 실습해보고 싶다.

