## 날짜: 2025-03-05

### 스크럼
- 학습 목표 1 : 밥팟 서비스 model max token 이슈 해결
- 학습 목표 2 : 밥팟 서비스 caching 구현

### 새로 배운 내용
#### 주제 1: 밥팟 서비스 model max token 이슈 해결
- 맥락 유지를 위해 state(지난 대화 내역 기록) 이용하고, 모델에게 제공했다.
- 이때, 유저의 대화 횟수가 늘자 모델에게 제공되는 텍스트 길이가 max token을 넘었다.
- trimer로 자를 수도 있지만, 어쩌피 state를 유지하는 이유가 모델에게 제공하기 위함이므로, trimer에서 자르는 길이 이상으로 저장하고 있을 필요도 없다고 판단했다.
- RemoveMessage 수정자를 이용해 저장되는 대화 개수를 조절했다.
- 장점:
    - 모델에게 제공하는 토큰 수가 줄어서 비용 절감
    - state는 인메모리 방식인데 동시에 여러 유저가 이용하는 경우, 메모리가 너무 무거워지는 것을 방지

#### 주제 2: 밥팟 서비스 caching 구현
- 해커톤 기간동안 챗봇 서비스를 이용하며 openai api 이용 요금이 상당했다.(체감상 4~5시간에 8천원?)
- 실제 배포하면 비용 부담이 예상되어 키워드 조합에 대해서는 미리 캐싱해두기로 결정.
- sqlite를 이용해서 서버 내 DB를 구축.
- 추후 키워드를 이용한 요청인 경우, 캐시된 데이터로 바로 답변하도록 DB 조희 작업을 workflow에 연결할 것.

### 오늘의 회고
- 실제 배포까지 생각하게 되니까 비용 문제 해결이 필요하다. 개발한 챗봇에서 불필요하게 토큰을 소비하는 부분을 찾아서 개선했다.
- 이외에도 식당 검색 성능을 위해서 데이터를 재수집해서 이용하는 방안 고려가 필요하다.
- 코드 리팩토링도 진행해야 한다. 챗봇을 이용하는 부분과 데이터베이스 구축하는 부분을 파일을 따로 분리하면 깔끔할 것 같다.