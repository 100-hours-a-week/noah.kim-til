## 날짜: 2025-03-04

### 스크럼
- 학습 목표 1 : 과적합(Overfitting)과 과소적합(Underfitting)의 개념과 판단 방법을 이해
- 학습 목표 2 : 모델 성능 비교 및 전이 학습(Transfer Learning)의 필요성을 학습
- 학습 목표 3 : 전이 학습을 활용한 모델의 학습 및 평가 방법

### 새로 배운 내용
#### 주제 1: 과적합(Overfitting)과 과소적합(Underfitting) 판단 방법
- 과적합(Overfitting): 학습 데이터에서는 성능이 뛰어나지만, 검증 데이터나 새로운 데이터에서는 성능이 떨어지는 현상.
- 과소적합(Underfitting): 학습 데이터에서도 성능이 충분히 나오지 않는 상태로, 모델이 데이터의 패턴을 제대로 학습하지 못한 경우.
- 과적합은 정규화를 통해 해결할 수 있고, 과소적합은 구조를 더 복잡하고 깊게 쌓음으로써 해결할 수 있다.

#### 주제 2: 모델 성능 비교와 선택 이유
- 다양한 모델을 비교하여 최적의 성능을 확보하는 것이 중요함.
- 전이 학습(Transfer Learning)**을 활용하면, 학습 시간을 단축하고 성능을 개선할 수 있음.

#### 주제 3: 전이 학습(Transfer Learning)
- 전이 학습(Transfer Learning): 사전 훈련된 모델을 그대로 사용하거나 추가 튜닝하여 새로운 문제에 적용함으로써 학습 시간을 단축하고 성능을 향상시키는 머신 러닝 기법.
- 전이 학습을 사용하는 이유
    - 데이터 절약:	충분한 데이터를 확보하기 어려운 경우, 사전 훈련된 모델의 지식을 활용
    - 시간 절약:	처음부터 모델을 훈련하는 것보다 사전 훈련된 모델을 활용하면 훨씬 빠름
    - 성능 향상:	사전 훈련된 모델은 다양한 데이터에서 학습했기 때문에 높은 성능을 보일 가능성이 큼
    - 계산 자원 절약:	모델을 처음부터 학습하는 데 필요한 계산 자원과 비용 절약 가능

### 오늘의 도전 과제와 해결 방법
- 도전 과제 1: 과적합 및 과소적합을 해결하는 방법 학습
    - 과적합 방지: 드롭아웃(Dropout) 추가, 데이터 증강(Augmentation), L2 정규화 적용
    - 과소적합 해결: 더 깊은 모델 사용, 데이터 양 증가, 학습률(LR) 조정
- 도전 과제 2: 도전 과제에 대한 설명 및 해결 방법
    - MNIST는 흑백(1채널)이므로 RGB(3채널) 변환 후 ResNet50에 입력
    - ResNet50은 224x224 해상도를 기대하므로 Resizing(224, 224) 적용
    - 전이 학습 적용(Fine-Tuning, Feature Extraction)
    - Fully Connected Layer 추가 + Dropout 적용하여 과적합 방지

### 오늘의 회고
- 과적합과 과소적합에 대한 개념을 명확히 이해할 수 있었다.
- 전이 학습(Transfer Learning)을 활용하면 적은 데이터로도 강력한 성능을 낼 수 있다는 점이 흥미로웠다.
- MNIST를 ResNet50을 사용해 학습하는 과정에서 모델 입력 형태를 변환하는 것이 중요함을 다시 한번 깨달았다.
- 다음에는 전이 학습을 더 다양한 데이터셋에서 실험해 보고 싶다.